// automatically generated by the FlatBuffers compiler, do not modify
extern crate flatbuffers;
use std::mem;
use std::cmp::Ordering;
use self::flatbuffers::{EndianScalar, Follow};
use super::*;
// struct MacAddress, aligned to 1
#[repr(transparent)]
#[derive(Clone, Copy, PartialEq)]
pub struct MacAddress(pub [u8; 6]);
impl Default for MacAddress { 
  fn default() -> Self { 
    Self([0; 6])
  }
}
impl std::fmt::Debug for MacAddress {
  fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
    f.debug_struct("MacAddress")
      .field("byte_0", &self.byte_0())
      .field("byte_1", &self.byte_1())
      .field("byte_2", &self.byte_2())
      .field("byte_3", &self.byte_3())
      .field("byte_4", &self.byte_4())
      .field("byte_5", &self.byte_5())
      .finish()
  }
}

impl flatbuffers::SimpleToVerifyInSlice for MacAddress {}
impl flatbuffers::SafeSliceAccess for MacAddress {}
impl<'a> flatbuffers::Follow<'a> for MacAddress {
  type Inner = &'a MacAddress;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    <&'a MacAddress>::follow(buf, loc)
  }
}
impl<'a> flatbuffers::Follow<'a> for &'a MacAddress {
  type Inner = &'a MacAddress;
  #[inline]
  fn follow(buf: &'a [u8], loc: usize) -> Self::Inner {
    flatbuffers::follow_cast_ref::<MacAddress>(buf, loc)
  }
}
impl<'b> flatbuffers::Push for MacAddress {
    type Output = MacAddress;
    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(self as *const MacAddress as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}
impl<'b> flatbuffers::Push for &'b MacAddress {
    type Output = MacAddress;

    #[inline]
    fn push(&self, dst: &mut [u8], _rest: &[u8]) {
        let src = unsafe {
            ::std::slice::from_raw_parts(*self as *const MacAddress as *const u8, Self::size())
        };
        dst.copy_from_slice(src);
    }
}

impl<'a> flatbuffers::Verifiable for MacAddress {
  #[inline]
  fn run_verifier(
    v: &mut flatbuffers::Verifier, pos: usize
  ) -> Result<(), flatbuffers::InvalidFlatbuffer> {
    use self::flatbuffers::Verifiable;
    v.in_buffer::<Self>(pos)
  }
}

impl<'a> MacAddress {
  #[allow(clippy::too_many_arguments)]
  pub fn new(
    byte_0: u8,
    byte_1: u8,
    byte_2: u8,
    byte_3: u8,
    byte_4: u8,
    byte_5: u8,
  ) -> Self {
    let mut s = Self([0; 6]);
    s.set_byte_0(byte_0);
    s.set_byte_1(byte_1);
    s.set_byte_2(byte_2);
    s.set_byte_3(byte_3);
    s.set_byte_4(byte_4);
    s.set_byte_5(byte_5);
    s
  }

  pub fn byte_0(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[0..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_0(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[0..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

  pub fn byte_1(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[1..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_1(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[1..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

  pub fn byte_2(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[2..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_2(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[2..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

  pub fn byte_3(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[3..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_3(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[3..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

  pub fn byte_4(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[4..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_4(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[4..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

  pub fn byte_5(&self) -> u8 {
    let mut mem = core::mem::MaybeUninit::<u8>::uninit();
    unsafe {
      core::ptr::copy_nonoverlapping(
        self.0[5..].as_ptr(),
        mem.as_mut_ptr() as *mut u8,
        core::mem::size_of::<u8>(),
      );
      mem.assume_init()
    }.from_little_endian()
  }

  pub fn set_byte_5(&mut self, x: u8) {
    let x_le = x.to_little_endian();
    unsafe {
      core::ptr::copy_nonoverlapping(
        &x_le as *const u8 as *const u8,
        self.0[5..].as_mut_ptr(),
        core::mem::size_of::<u8>(),
      );
    }
  }

}

